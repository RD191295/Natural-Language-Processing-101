{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NLTK Library\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenizer & Sentence Tokennizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"my name is raj dalsaniya. I am R&D engineer. I have intersted in data science and AI.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenizer\n",
    "\n",
    "#### Word Tokenization is the process of breaking down a sentence or text into individual words, called tokens. It helps in natural language processing (NLP) by making text easier to analyze and process.\n",
    "\n",
    "#### For example, given the sentence: \n",
    "\"Hello! How are you?\"\n",
    "Word tokenization splits it into:\n",
    "['Hello', '!', 'How', 'are', 'you', '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'raj', 'dalsaniya', '.', 'I', 'am', 'R', '&', 'D', 'engineer', '.', 'I', 'have', 'intersted', 'in', 'data', 'science', 'and', 'AI', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "#text = \"Hello! How are you?\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Sentence Tokenization\n",
    "\n",
    "#### Sentence Tokenization (or Sentence Segmentation) is the process of dividing a text into individual sentences. It helps in natural language processing (NLP) by breaking down large texts into manageable parts.\n",
    "\n",
    "### For example, given the text:\n",
    "ðŸ‘‰ \"Hello! How are you? I hope you're doing well.\"\n",
    "\n",
    "### Sentence tokenization splits it into:\n",
    "âœ… [\"Hello!\", \"How are you?\", \"I hope you're doing well.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my name is raj dalsaniya.', 'I am R&D engineer.', 'I have intersted in data science and AI.']\n"
     ]
    }
   ],
   "source": [
    "sent_token = sent_tokenize(text)\n",
    "print(sent_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords\n",
    "\n",
    "#### Stopwords are common words (like \"is\", \"the\", \"and\", \"in\") that do not add much meaning to a sentence and are often removed during text processing to improve efficiency in NLP tasks like text analysis and search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = text.lower()\n",
    "\n",
    "# Tokenize words\n",
    "tokens = word_tokenize(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = set(stopwords.words('english'))  # Use set for faster lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'raj', 'dalsaniya', '.', 'r', '&', 'engineer', '.', 'intersted', 'data', 'science', 'ai', '.']\n"
     ]
    }
   ],
   "source": [
    "filter_text = [word for word in tokens if word not in stopwords_list]\n",
    "\n",
    "\n",
    "# Print filter text after stop words\n",
    "print(filter_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Stemming in NLP\n",
    "#### Stemming is the process of reducing words to their root form by removing suffixes. It helps in normalizing words, making NLP tasks more efficient.\n",
    "\n",
    "#### For example:\n",
    "âœ… \"running\" â†’ \"run\"\n",
    "âœ… \"happily\" â†’ \"happili\" (not always a real word)\n",
    "\n",
    "#### Stemming is faster but sometimes produces non-dictionary words because it simply chops off suffixes without understanding meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is raj dalsaniya . i am r & d engin . i have interst in data scienc and ai . "
     ]
    }
   ],
   "source": [
    "stemming = PorterStemmer()\n",
    "text = \"my name is raj dalsaniya. I am R&D engineer. I have intersted in data science and AI.\"\n",
    "\n",
    "for i in word_tokenize(text):\n",
    "    print(stemming.stem(i),end =\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer\n",
    "\n",
    "#### A lemmatizer is a tool or algorithm that reduces words to their base or lemma form. Unlike a stemmer, which simply chops off affixes (like \"-ing\" or \"-ed\"), a lemmatizer considers the context and part of speech to return a proper dictionary word.\n",
    "\n",
    "#### For example:\n",
    "\n",
    "#### <li>Running â†’ run </li>\n",
    "#### <li>Better â†’ good (lemmatization considers word meaning) </li>\n",
    "#### <li>Was â†’ be </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\rajda/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i am teaching in my classes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am teaching in my class "
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(text):\n",
    "    print(lem.lemmatize(i), end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer - lancasterstemer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bet'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lanc = LancasterStemmer()\n",
    "\n",
    "lanc.stem('running')\n",
    "\n",
    "lanc.stem('Better')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tag (Part of Speech Tagging)\n",
    "\n",
    "#### POS tagging is the process of labeling words in a sentence with their correct part of speech (noun, verb, adjective, etc.) based on context.\n",
    "\n",
    "## Example:\n",
    "### Sentence:\n",
    "#### ðŸ‘‰ \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "### POS Tags:\n",
    "\n",
    "#### <li>The â†’ Determiner (DT) </li>\n",
    "#### <li>quick â†’ Adjective (JJ) </li>\n",
    "#### <li>brown â†’ Adjective (JJ) </li>\n",
    "#### <li>fox â†’ Noun (NN) </li>\n",
    "#### <li>jumps â†’ Verb (VBZ) </li>\n",
    "#### <li>over â†’ Preposition (IN) </li>\n",
    "#### <li>the â†’ Determiner (DT) </li>\n",
    "#### <li>lazy â†’ Adjective (JJ) </li>\n",
    "#### <li>dog â†’ Noun (NN) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('teaching', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('classes', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
