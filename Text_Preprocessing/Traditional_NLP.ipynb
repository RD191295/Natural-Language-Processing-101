{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"hello Marry, don't slap the green witch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'marry', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch', '!']\n"
     ]
    }
   ],
   "source": [
    "print([str(token) for token in nlp(text.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\rajda\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp38-cp38-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rajda\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\rajda\\anaconda3\\envs\\myenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading regex-2024.9.11-cp38-cp38-win_amd64.whl (274 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1 regex-2024.9.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet=\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['snow',\n",
       " 'white',\n",
       " 'and',\n",
       " 'the',\n",
       " 'seven',\n",
       " 'degrees',\n",
       " '#makeamoviecold',\n",
       " '@midnight',\n",
       " ':)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_tweet = TweetTokenizer()\n",
    "\n",
    "token_tweet.tokenize(tweet.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams, Bigrams, Trigrams, …, N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'marry'],\n",
       " ['marry', ','],\n",
       " [',', 'do'],\n",
       " ['do', \"n't\"],\n",
       " [\"n't\", 'slap'],\n",
       " ['slap', 'the'],\n",
       " ['the', 'green'],\n",
       " ['green', 'witch'],\n",
       " ['witch', '!']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def n_grams(text, n) :\n",
    "    \"\"\"\n",
    "        takes tokens or text, returns a list of ngrams \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return [text[i : i+n] for i in range(len(text) - n+1)]\n",
    "\n",
    "\n",
    "\n",
    "text = \"hello Marry, don't slap the green witch!\"\n",
    "\n",
    "cleaned_text = [str(token) for token in nlp(text.lower())]\n",
    "\n",
    "n_grams(cleaned_text, 2) # Biagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello'],\n",
       " ['marry'],\n",
       " [','],\n",
       " ['do'],\n",
       " [\"n't\"],\n",
       " ['slap'],\n",
       " ['the'],\n",
       " ['green'],\n",
       " ['witch'],\n",
       " ['!']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams(cleaned_text, 1) # Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'marry', ','],\n",
       " ['marry', ',', 'do'],\n",
       " [',', 'do', \"n't\"],\n",
       " ['do', \"n't\", 'slap'],\n",
       " [\"n't\", 'slap', 'the'],\n",
       " ['slap', 'the', 'green'],\n",
       " ['the', 'green', 'witch'],\n",
       " ['green', 'witch', '!']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams(cleaned_text, 3) # trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas and Stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he ---> he\n",
      "was ---> be\n",
      "running ---> run\n",
      "late ---> late\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization: reducing words to their root forms\n",
    "doc = nlp(u\"he was running late geese.”\")\n",
    "for token in doc:\n",
    "    print('{} ---> {}'.format(token, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programmer  :  programm\n",
      "programming  :  program\n",
      "was  :  wa\n",
      "geese  :  gees\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "ps = PorterStemmer()\n",
    " \n",
    "# choose some words to be stemmed\n",
    "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"was\",\"geese\"]\n",
    " \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing Sentences and Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello -----> INTJ\n",
      "Marry -----> PROPN\n",
      ", -----> PUNCT\n",
      "do -----> AUX\n",
      "n't -----> PART\n",
      "slap -----> VERB\n",
      "the -----> DET\n",
      "green -----> ADJ\n",
      "witch -----> NOUN\n",
      "! -----> PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print('{} -----> {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello Marry ----> NP\n",
      "the green witch ----> NP\n"
     ]
    }
   ],
   "source": [
    "### Categorizing Spans: Chunking and Named Entity Recognition\n",
    "### Noun Phrase (NP) chunking\n",
    "doc = nlp(text)\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print('{} ----> {}'.format(chunk,chunk.label_))\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
